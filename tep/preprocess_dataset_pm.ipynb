{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import *\n",
    "import os\n",
    "import pyreadr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM,Conv2D,Conv1D,Flatten,MaxPooling2D,GlobalAveragePooling2D,Dropout,Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "import keras_nlp\n",
    "import keras_nlp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER_PATH = \"dataset/RData\"\n",
    "\n",
    "# faulty_train = torch.load(os.path.join(TORCH_DATASET_FOLDER_PATH, \"TEP_Faulty_Training.torch\"), weights_only= True)\n",
    "# faulty_test = torch.load(os.path.join(TORCH_DATASET_FOLDER_PATH, \"TEP_Faulty_Testing.torch\"), weights_only= True)\n",
    "# faultfree_train = torch.load(os.path.join(TORCH_DATASET_FOLDER_PATH, \"TEP_FaultFree_Training.torch\"), weights_only= True)\n",
    "# faultfree_test = torch.load(os.path.join(TORCH_DATASET_FOLDER_PATH, \"TEP_FaultFree_Testing.torch\"), weights_only= True)\n",
    "faulty_train_df = pyreadr.read_r(os.path.join(DATASET_FOLDER_PATH, \"TEP_Faulty_Training.RData\"))[\"faulty_training\"]\n",
    "\n",
    "faultfree_train_df = pyreadr.read_r(os.path.join(DATASET_FOLDER_PATH, \"TEP_FaultFree_Training.RData\"))[\"fault_free_training\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((faultfree_train_df, faulty_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faultNumber</th>\n",
       "      <th>simulationRun</th>\n",
       "      <th>sample</th>\n",
       "      <th>xmeas_1</th>\n",
       "      <th>xmeas_2</th>\n",
       "      <th>xmeas_3</th>\n",
       "      <th>xmeas_4</th>\n",
       "      <th>xmeas_5</th>\n",
       "      <th>xmeas_6</th>\n",
       "      <th>xmeas_7</th>\n",
       "      <th>...</th>\n",
       "      <th>xmv_2</th>\n",
       "      <th>xmv_3</th>\n",
       "      <th>xmv_4</th>\n",
       "      <th>xmv_5</th>\n",
       "      <th>xmv_6</th>\n",
       "      <th>xmv_7</th>\n",
       "      <th>xmv_8</th>\n",
       "      <th>xmv_9</th>\n",
       "      <th>xmv_10</th>\n",
       "      <th>xmv_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25038</td>\n",
       "      <td>3674.0</td>\n",
       "      <td>4529.0</td>\n",
       "      <td>9.2320</td>\n",
       "      <td>26.889</td>\n",
       "      <td>42.402</td>\n",
       "      <td>2704.3</td>\n",
       "      <td>...</td>\n",
       "      <td>53.744</td>\n",
       "      <td>24.657</td>\n",
       "      <td>62.544</td>\n",
       "      <td>22.137</td>\n",
       "      <td>39.935</td>\n",
       "      <td>42.323</td>\n",
       "      <td>47.757</td>\n",
       "      <td>47.510</td>\n",
       "      <td>41.258</td>\n",
       "      <td>18.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25109</td>\n",
       "      <td>3659.4</td>\n",
       "      <td>4556.6</td>\n",
       "      <td>9.4264</td>\n",
       "      <td>26.721</td>\n",
       "      <td>42.576</td>\n",
       "      <td>2705.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.414</td>\n",
       "      <td>24.588</td>\n",
       "      <td>59.259</td>\n",
       "      <td>22.084</td>\n",
       "      <td>40.176</td>\n",
       "      <td>38.554</td>\n",
       "      <td>43.692</td>\n",
       "      <td>47.427</td>\n",
       "      <td>41.359</td>\n",
       "      <td>17.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25038</td>\n",
       "      <td>3660.3</td>\n",
       "      <td>4477.8</td>\n",
       "      <td>9.4426</td>\n",
       "      <td>26.875</td>\n",
       "      <td>42.070</td>\n",
       "      <td>2706.2</td>\n",
       "      <td>...</td>\n",
       "      <td>54.357</td>\n",
       "      <td>24.666</td>\n",
       "      <td>61.275</td>\n",
       "      <td>22.380</td>\n",
       "      <td>40.244</td>\n",
       "      <td>38.990</td>\n",
       "      <td>46.699</td>\n",
       "      <td>47.468</td>\n",
       "      <td>41.199</td>\n",
       "      <td>20.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.24977</td>\n",
       "      <td>3661.3</td>\n",
       "      <td>4512.1</td>\n",
       "      <td>9.4776</td>\n",
       "      <td>26.758</td>\n",
       "      <td>42.063</td>\n",
       "      <td>2707.2</td>\n",
       "      <td>...</td>\n",
       "      <td>53.946</td>\n",
       "      <td>24.725</td>\n",
       "      <td>59.856</td>\n",
       "      <td>22.277</td>\n",
       "      <td>40.257</td>\n",
       "      <td>38.072</td>\n",
       "      <td>47.541</td>\n",
       "      <td>47.658</td>\n",
       "      <td>41.643</td>\n",
       "      <td>18.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.29405</td>\n",
       "      <td>3679.0</td>\n",
       "      <td>4497.0</td>\n",
       "      <td>9.3381</td>\n",
       "      <td>26.889</td>\n",
       "      <td>42.650</td>\n",
       "      <td>2705.1</td>\n",
       "      <td>...</td>\n",
       "      <td>53.658</td>\n",
       "      <td>28.797</td>\n",
       "      <td>60.717</td>\n",
       "      <td>21.947</td>\n",
       "      <td>39.144</td>\n",
       "      <td>41.955</td>\n",
       "      <td>47.645</td>\n",
       "      <td>47.346</td>\n",
       "      <td>41.507</td>\n",
       "      <td>18.461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   faultNumber  simulationRun  sample  xmeas_1  xmeas_2  xmeas_3  xmeas_4  \\\n",
       "0          1.0            1.0       1  0.25038   3674.0   4529.0   9.2320   \n",
       "1          1.0            1.0       2  0.25109   3659.4   4556.6   9.4264   \n",
       "2          1.0            1.0       3  0.25038   3660.3   4477.8   9.4426   \n",
       "3          1.0            1.0       4  0.24977   3661.3   4512.1   9.4776   \n",
       "4          1.0            1.0       5  0.29405   3679.0   4497.0   9.3381   \n",
       "\n",
       "   xmeas_5  xmeas_6  xmeas_7  ...   xmv_2   xmv_3   xmv_4   xmv_5   xmv_6  \\\n",
       "0   26.889   42.402   2704.3  ...  53.744  24.657  62.544  22.137  39.935   \n",
       "1   26.721   42.576   2705.0  ...  53.414  24.588  59.259  22.084  40.176   \n",
       "2   26.875   42.070   2706.2  ...  54.357  24.666  61.275  22.380  40.244   \n",
       "3   26.758   42.063   2707.2  ...  53.946  24.725  59.856  22.277  40.257   \n",
       "4   26.889   42.650   2705.1  ...  53.658  28.797  60.717  21.947  39.144   \n",
       "\n",
       "    xmv_7   xmv_8   xmv_9  xmv_10  xmv_11  \n",
       "0  42.323  47.757  47.510  41.258  18.447  \n",
       "1  38.554  43.692  47.427  41.359  17.194  \n",
       "2  38.990  46.699  47.468  41.199  20.530  \n",
       "3  38.072  47.541  47.658  41.643  18.089  \n",
       "4  41.955  47.645  47.346  41.507  18.461  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"faultNumber\"] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 500, 500, 55)\n",
      "4500000\n"
     ]
    }
   ],
   "source": [
    "# processed_data = df.loc[df[\"sample\"] <= 40]\n",
    "processed_data = df\n",
    "processed_data = processed_data[processed_data[\"faultNumber\"] != 3]\n",
    "processed_data = processed_data[processed_data[\"faultNumber\"] != 9]\n",
    "processed_data = processed_data[processed_data[\"faultNumber\"] != 15]\n",
    "\n",
    "np_dataset = processed_data.to_numpy().reshape((len(pd.unique(processed_data[\"faultNumber\"])),\n",
    "                                                len(pd.unique(processed_data[\"simulationRun\"])),\n",
    "                                                len(pd.unique(processed_data[\"sample\"])),\n",
    "                                                55))\n",
    "print(np_dataset.shape)\n",
    "print(len(processed_data))\n",
    "# processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 500, 48, 20, 52)\n",
      "(18, 500, 48, 1)\n",
      "[0.25038 0.25109 0.25038 0.24977 0.29405 0.29303 0.24301 0.2409  0.29416\n",
      " 0.29372 0.2348  0.2357  0.25649 0.25909 0.2908  0.29176 0.23719 0.23812\n",
      " 0.25179 0.25299]\n"
     ]
    }
   ],
   "source": [
    "sample_count = 500\n",
    "width = 20\n",
    "stride = 10\n",
    "lookahead = 5\n",
    "start_idx_range = range(0, sample_count-width-lookahead, stride)\n",
    "\n",
    "x_shape = list(np_dataset.shape)\n",
    "x_shape[-2] = len(start_idx_range)\n",
    "x_shape.insert(-1, width)\n",
    "y_shape = list(x_shape)\n",
    "\n",
    "x_shape[-1] = 52\n",
    "y_shape = y_shape[:-1]\n",
    "y_shape[-1] = 1\n",
    "\n",
    "x_dataset = np.zeros(x_shape, dtype=np.float32)\n",
    "y_dataset = np.zeros(y_shape, dtype=np.float32)\n",
    "print(x_dataset.shape)\n",
    "print(y_dataset.shape)\n",
    "idx = 0\n",
    "\n",
    "for start_idx in start_idx_range:\n",
    "    x_dataset[:, :, idx, :, :] = np_dataset[:, :, start_idx:start_idx+width, 3:]\n",
    "    y_dataset[:, :, idx, :] = np_dataset[:, :, start_idx+width+lookahead, 0:1]\n",
    "    idx += 1\n",
    "# data.head(50)\n",
    "# print(x_dataset.shape)\n",
    "print(x_dataset[1, 0, 0, :, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(432000, 20, 52)\n",
      "(432000, 1)\n",
      "(432000, 20, 52)\n",
      "(432000, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "reshaped_x_dataset = x_dataset.reshape((-1, *x_dataset.shape[-2:]))\n",
    "reshaped_y_dataset = y_dataset.reshape((-1, y_dataset.shape[-1]))\n",
    "\n",
    "print(reshaped_x_dataset.shape)\n",
    "print(reshaped_y_dataset.shape)\n",
    "\n",
    "sc = StandardScaler()\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "x_scaled = sc.fit_transform(reshaped_x_dataset.reshape(-1, reshaped_x_dataset.shape[-1])).reshape(reshaped_x_dataset.shape)\n",
    "y_enc = enc.fit_transform(reshaped_y_dataset)\n",
    "\n",
    "print(x_scaled.shape)\n",
    "print(y_enc.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_scaled.astype(np.float32), y_enc.astype(np.float32), train_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset/dataset_pm.npy\", \"wb\") as f:\n",
    "    print(x_train.dtype)\n",
    "    print(y_train.dtype)\n",
    "    print(x_val.dtype)\n",
    "    print(y_val.dtype)\n",
    "    np.save(f, x_train)\n",
    "    np.save(f, y_train)\n",
    "    np.save(f, x_val)\n",
    "    np.save(f, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras_nlp.layers' has no attribute 'SinePositionEncoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# model = LSTM_model(x_train,y_train)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Train the model with 20 epochs and batch size of 32, using the early stopping callback\u001b[39;00m\n\u001b[0;32m     36\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(x_val, y_val), callbacks\u001b[38;5;241m=\u001b[39m[early_stop])\n",
      "Cell \u001b[1;32mIn[10], line 25\u001b[0m, in \u001b[0;36mTransformer_model\u001b[1;34m(x_train, y_train, num_layers, embedding_dim, num_heads)\u001b[0m\n\u001b[0;32m     23\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(sequence_length,input_size))\n\u001b[0;32m     24\u001b[0m x_embedding \u001b[38;5;241m=\u001b[39m Dense(embedding_dim)(input_layer)\n\u001b[1;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m x_embedding \u001b[38;5;241m+\u001b[39m \u001b[43mkeras_nlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSinePositionEncoding\u001b[49m()(x_embedding)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers):\n\u001b[0;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mTransformerEncoder(embedding_dim, num_heads, \u001b[38;5;241m0.5\u001b[39m)(x)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras_nlp.layers' has no attribute 'SinePositionEncoding'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def LSTM_model(X_train,y_train):\n",
    "    # Define input layer\n",
    "    input_layer = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
    "\n",
    "    # Define encoder layers\n",
    "    encoded = Bidirectional(LSTM(64,activation=\"tanh\",return_sequences=True))(input_layer)\n",
    "    encoded = LSTM(64, activation=\"tanh\")(encoded)\n",
    "\n",
    "    # Define decoder layers\n",
    "    decoded = Dense(128, activation='selu')(encoded)\n",
    "    decoded = Dropout(0.5)(decoded)\n",
    "    decoded = Dense(y_train.shape[1], activation='softmax')(decoded)\n",
    "\n",
    "    # Define LSTM model\n",
    "    lstm_model = Model(inputs=input_layer, outputs=decoded)\n",
    "    # Compile LSTM  model\n",
    "    lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return lstm_model\n",
    "\n",
    "def Transformer_model(x_train, y_train, num_layers, embedding_dim, num_heads):\n",
    "    sequence_length = x_train.shape[-2]\n",
    "    input_size = x_train.shape[-1]\n",
    "    input_layer = Input(shape=(sequence_length,input_size))\n",
    "    x_embedding = Dense(embedding_dim)(input_layer)\n",
    "    x = x_embedding + keras_nlp.layers.SinePositionEncoding()(x_embedding)\n",
    "    for i in range(num_layers):\n",
    "        x = keras_nlp.layers.TransformerEncoder(embedding_dim, num_heads, 0.5)(x)\n",
    "    print(x.shape)\n",
    "\n",
    "# Define early stopping callback to monitor validation loss and stop if it doesn't improve for 5 epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# Create model\n",
    "# model = LSTM_model(x_train,y_train)\n",
    "model = Transformer_model(x_train, y_train, num_layers=8, embedding_dim=512, num_heads=8)\n",
    "# Train the model with 20 epochs and batch size of 32, using the early stopping callback\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=256, validation_data=(x_val, y_val), callbacks=[early_stop])\n",
    "\n",
    "# Plot the training history for loss and accuracy\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PositionEmbeddingFixedWeights' from 'keras.layers' (c:\\Batu\\Projects\\fault-detection-kocsistem\\.conda\\Lib\\site-packages\\keras\\api\\layers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LayerNormalization, Layer, Dense, ReLU, Dropout\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiHeadAttention\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PositionEmbeddingFixedWeights\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Implementing the Add & Norm Layer\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAddNormalization\u001b[39;00m(Layer):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'PositionEmbeddingFixedWeights' from 'keras.layers' (c:\\Batu\\Projects\\fault-detection-kocsistem\\.conda\\Lib\\site-packages\\keras\\api\\layers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
    "from keras.layers import MultiHeadAttention\n",
    "from keras.layers import PositionEmbeddingFixedWeights\n",
    "\n",
    "\n",
    "\n",
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AddNormalization, self).__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "\n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)\n",
    "\n",
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    "\n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "\n",
    "        return self.fully_connected2(self.activation(x_fc1))\n",
    "\n",
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)\n",
    "\n",
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
